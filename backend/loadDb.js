import { DataAPIClient } from "@datastax/astra-db-ts";
import OpenAI from "openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import "dotenv/config";
import { scrapeData } from "./scraper.js";
import pLimit from "p-limit";

// ENV variables
const {
  ASTRA_DB_NAMESPACE,
  ASTRA_DB_COLLECTION,
  ASTRA_DB_API_ENDPOINT,
  ASTRA_DB_APPLICATION_TOKEN,
  AI_API_KEY,
} = process.env;

// OpenAI instance
const openai = new OpenAI({ apiKey: AI_API_KEY });

// Astra DB setup
const client = new DataAPIClient(ASTRA_DB_APPLICATION_TOKEN);
const db = client.db(ASTRA_DB_API_ENDPOINT, { namespace: ASTRA_DB_NAMESPACE });

// Text splitter config
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 512,
  chunkOverlap: 100,
});

// âœ… Exported function to create a user collection
const createUserCollection = async (collectionName) => {
    const collections = await db.listCollections();
    const exists = collections.find((c) => c.name === collectionName);
  if (!exists) {
    await db.createCollection(collectionName);
    console.log("âœ… User Collection created.");
  } else {
    console.log("â„¹ï¸ User Collection already exists.");
  }
}

// âœ… Exported function to create the collection
const createCollection = async (collectionName) => {
  const collections = await db.listCollections();
  const exists = collections.find((c) => c.name === collectionName);
  if (!exists) {
    await db.createCollection(collectionName, {
      vector: {
        dimension: 1536,
        metric: "dot_product",
      },
    });
    console.log("âœ… Collection created.");
  } else {
    console.log("â„¹ï¸ Collection already exists.");
  }
};

// âœ… Exported function to load data
const loadSampleData = async ({ wipe = false } = {}) => {
  const collection = await db.collection(ASTRA_DB_COLLECTION);

  if (wipe) {
    await collection.deleteMany({});
    console.log("ğŸ§¹ Collection wiped.");
  }

  const scraped = await scrapeData(); // { heading: { text, url } }
  const data = scraped?.nursingData ?? scraped;
  const timestamp = new Date().toISOString();

  const limit = pLimit(5); // â¬…ï¸ Limit to 5 concurrent embedding requests

  let headingCount = 0;
  const totalHeadings = Object.keys(data).length;

  for (const [heading, value] of Object.entries(data)) {
    const text = typeof value === "string" ? value : value.text;
    const url = typeof value === "string" ? null : value.url;

    if (!text || typeof text !== "string" || !text.trim()) {
      console.warn(`âš ï¸ Skipping invalid entry: ${heading}`);
      continue;
    }

    const chunks = await splitter.splitText(text);

    // TODO: this if statement skips over the pdfs with too many chunks. Always: "PDF: 2025-2026 Nursing Student Handbook". Delete this statement when the PDF problem is fixed
    if (chunks.length > 100) {
      console.warn(`âš ï¸ "${heading}" has ${chunks.length} chunks. Skipping for now.`);
      continue;
    }

    // Build embedding tasks with concurrency limit
    const embeddingTasks = chunks.map(chunk =>
      limit(async () => {
        const embedding = await openai.embeddings.create({
          model: "text-embedding-3-small",
          input: chunk,
          encoding_format: "float",
        });
        return {
          chunk,
          vector: embedding.data[0].embedding,
        };
      })
    );


    // Wait for embeddings to finish
    const embeddedChunks = await Promise.all(embeddingTasks);

    // Build documents for bulk insert
    const documents = embeddedChunks.map((item, i) => ({
      _id: `${heading}_${i}`.replace(/\s+/g, "_"),
      $vector: item.vector,
      heading,
      text: item.chunk,
      url,
      timestamp,
    }));

    if (documents.length > 0) {
      await collection.insertMany(documents);
      headingCount++;
      console.log(`ğŸ“¦ [${headingCount}/${totalHeadings}] Inserted ${documents.length} chunks for "${heading}" from ${url}`);
    }
  }

  console.log("âœ… Data load complete.");
};


// âŒ No automatic execution here anymore!

// âœ… Only exports
export { createCollection, createUserCollection, loadSampleData };
